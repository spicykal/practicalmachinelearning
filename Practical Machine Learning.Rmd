---
title: "Practical Machine Learning"
author: "Calvin Hutto"
date: "March 26, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of ddata about personal activity relatively inexpensively. These type of devices are part of the quantified selfe movement of a group of enthusiasts who take measurements about themselves regularly to imporove their health, to find patterns in their behavior, or because they are tech geeks.

In this project, the goal will be to use data from accelerometers on the belt, foreamr, arm and dumbell of 6 participants to predict the manner in which participants did the exercise.

##Loading Appropriate Packages

Loading appropriate packages and their associated dependencies is a crucial step in any R programming endeavour.
```{r Packages}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

In order to make the results of this assignment reproducible, I set a random number seed at the beginning of the code.  Its my birthday!

```{r Seed}
set.seed(102278)
```
## Data

These lines of code essentially get the data from a URL and then assign the data to "training" and "testing" sets.

```{r Nike}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```
The next step is to parse the data in to training and tests sets and to set the appropriate working directory.

```{r Reebok}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```
## Cleaning the data

Within the data, there are several variables with near zero variance.  We want to remove these from the analysis as they don't provide any usefull information to our problem set.

```{r Adidas}
myNZVariance <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVariance]
```
We also want to remove the first column of the data set as it is ID and has no meaningful input to the desired outcome of this assignment.  The following code accomplishes that.

```{r UA}
myTraining <- myTraining[c(-1)]
```
The last step I took in order to clean the data was to remove columns with too many NAs.  So if there were more than 60% NAs in a given column, we will not use said column in our analysis.

```{r Puma}
trainingV3 <- myTraining 
for(i in 1:length(myTraining)) { 
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { 
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { 
                trainingV3 <- trainingV3[ , -j] 
            }   
        } 
    }
}
```
```{r Puma1}
myTraining <- trainingV3
rm(trainingV3)

clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])
myTesting <- myTesting[clean1]
testing <- testing[clean2]
```
Coercing all data into the same type is required in order for the machine learning algorithms to function properly.

```{r Puma2}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
```
##Initial Machine Learning Algorithm: Decision Tree

This bit of code fits our decision tree model and displays a "fancy" decision tree graphic.

```{r Clarks}
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
```

Next we attempt to make prediction on our testing set of data and create a confusion matrix to determine our accuracy.  In other words, "How did our model work?"

```{r MB}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
confusionMatrix(predictionsA1, myTesting$classe)
```

This output suggests that the overall accuracy of our model was about 88%.  Thats not bad but I think we can do better.

##Machine Learning with Random Forest

As random forest models generally produce more accurate results, these next few lines of code fit a random forest model to our set of training data and then attempt to make predictions about the test set.

```{r Merril}
modFitB1 <- randomForest(classe ~. , data=myTraining)
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
confusionMatrix(predictionsB1, myTesting$classe)
```

As expected, the random forest model produced a model that predicted the outcome 99% of the time correctly.  This is a pretty solid model.

##Generating Answer Files

Lastly we apply our fitted random forest model to the provided test set and generate answers for submission during the quiz.

```{r oakley, message=FALSE, warning=FALSE}
#predictionsB2 <- predict(modFitB1, testing, type = "class")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

#pml_write_files(predictionsB2)
```
I hope this assignment is easy to grade!  BTW, I couldn't get my predictionB2 to knit to HTML. R wrote the files to my working directory with assignment answers and got 100% on the quiz...